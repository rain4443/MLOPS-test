import os, io, time, uuid, numpy as np, streamlit as st
from utils.config import (AWS_REGION, S3_BUCKET, EMBED_MODEL_ID, LLM_MODEL_ID,
                          CHUNK_SIZE, CHUNK_OVERLAP, TOP_K, LOCAL_CACHE_DIR)
from utils import s3 as s3util
from rag.loader import load_pdf_bytes_to_text
from rag.chunker import chunk_text
from rag.embedder import bedrock_embed
from rag.faiss_store import build_faiss, save_local, upload_to_s3, ensure_local_index
from rag.retriever import search_topk, build_context
from rag.generator import make_question_prompt, invoke_llm

st.set_page_config(page_title="ë¬¸ì œì§‘ AI (RAG@Bedrock)", layout="wide")
st.title("ğŸ“˜ í•™ì—…ì˜ ì§„ì‹¬ì´ ë‚˜ë¼ë¥¼ ìœ„í•œ ë¬¸ì œì§‘ AI")

with st.sidebar:
    st.subheader("í™˜ê²½")
    st.write(f"AWS Region: `{AWS_REGION}`")
    st.write(f"S3 Bucket: `{S3_BUCKET}`")
    st.write(f"Embed: `{EMBED_MODEL_ID}`")
    st.write(f"LLM: `{LLM_MODEL_ID}`")
    st.divider()
    st.caption("ğŸ”’ ì—…ë¡œë“œëœ PDFì™€ ì„ë² ë”© ì¸ë±ìŠ¤ëŠ” S3ì— ì €ì¥ë©ë‹ˆë‹¤.")

tab1, tab2 = st.tabs(["1) ë¬¸ì„œ ì—…ë¡œë“œ & ì¸ë±ì‹±", "2) ê²€ìƒ‰/ì§ˆì˜ & ë¬¸ì œ ìƒì„±"])

# ===== 1) ì—…ë¡œë“œ & ì„ë² ë”©/FAISS =====
with tab1:
    st.header("ë¬¸ì„œ ì—…ë¡œë“œ ë° ì¸ë±ìŠ¤ ìƒì„±")
    pdf_file = st.file_uploader("PDF ì—…ë¡œë“œ", type=["pdf"])
    colA, colB = st.columns(2)
    with colA:
        chunk_size = st.number_input("Chunk Size", 200, 2000, CHUNK_SIZE, step=50)
    with colB:
        chunk_overlap = st.number_input("Chunk Overlap", 0, 500, CHUNK_OVERLAP, step=10)

    if st.button("ì¸ë±ìŠ¤ ìƒì„±", disabled=(pdf_file is None)):
        with st.spinner("PDF ì²˜ë¦¬ ì¤‘..."):
            pdf_bytes = pdf_file.read()
            text = load_pdf_bytes_to_text(pdf_bytes)

        with st.spinner("ì²­í¬ ë¶„í• ..."):
            cobj = chunk_text(text, size=int(chunk_size), overlap=int(chunk_overlap))
            chunks, metas = cobj["chunks"], cobj

        with st.spinner("ì„ë² ë”© & ì¸ë±ìŠ¤ ìƒì„±(Bedrock)..."):
            # ë°°ì¹˜ ë¶„í• ë¡œ ì„ë² ë”© (ê°„ë‹¨ ë²„ì „: í†µì§œ í˜¸ì¶œ)
            embs = bedrock_embed(chunks, EMBED_MODEL_ID).astype("float32")
            # ì •ê·œí™”(ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            from faiss import normalize_L2
            normalize_L2(embs)
            index = build_faiss(embs)

        # ë¬¸ì„œ ID ìƒì„± ë° S3 ì €ì¥
        doc_id = uuid.uuid4().hex
        cache_dir = os.path.join(LOCAL_CACHE_DIR, doc_id)
        # ë©”íƒ€ì— ì›ë³¸ chunks í¬í•¨(ê°„ë‹¨í™”)
        metas["chunks"] = chunks

        with st.spinner("S3 ì €ì¥ ì¤‘..."):
            # ì›ë¬¸ PDF ì €ì¥
            s3util._s3.put_object(Bucket=S3_BUCKET,
                                  Key=f"pdfs/{doc_id}/input.pdf", Body=pdf_bytes)
            # FAISS + meta ì €ì¥
            save_local(index, metas, cache_dir)
            upload_to_s3(cache_dir, f"faiss/{doc_id}")

        st.success("ì¸ë±ìŠ¤ ìƒì„± & ì—…ë¡œë“œ ì™„ë£Œ!")
        st.code(f"doc_id = {doc_id}", language="bash")
        st.session_state["last_doc_id"] = doc_id

# ===== 2) ê²€ìƒ‰/ì§ˆì˜ =====
with tab2:
    st.header("ê²€ìƒ‰/ì§ˆì˜ & ë¬¸ì œ ìƒì„±")
    doc_id = st.text_input("doc_id ì…ë ¥(ë˜ëŠ” ì¢Œì¸¡ íƒ­ì—ì„œ ìƒì„± í›„ ìë™ ì±„ì›€)",
                           value=st.session_state.get("last_doc_id", ""))

    query = st.text_area("ìš”ì²­(ì˜ˆ: 'ì¤‘ìš” ê°œë… 3ê°œì— ëŒ€í•´ 5ì§€ì„ ë‹¤ 2ë¬¸ì œì”© ë§Œë“¤ì–´ì¤˜')", height=120)

    c1, c2, c3 = st.columns(3)
    with c1:
        topk = st.number_input("Top-K", 1, 20, TOP_K)
    with c2:
        max_tokens = st.number_input("Max Tokens", 200, 4000, 1000, step=100)
    with c3:
        temperature = st.slider("Temperature", 0.0, 1.0, 0.2, 0.05)

    if st.button("ê²€ìƒ‰ & ìƒì„± ì‹¤í–‰", disabled=(len(doc_id.strip()) == 0 or len(query.strip()) == 0)):
        try:
            with st.spinner("ì¸ë±ìŠ¤ ë¡œë”©(S3 ìºì‹œ)â€¦"):
                index, meta = ensure_local_index(doc_id, "faiss")
                chunks = meta["chunks"]

            with st.spinner("Top-K ê²€ìƒ‰â€¦"):
                idxs, scores = search_topk(query, index, meta, k=int(topk))
                ctx = build_context(meta, idxs, chunks)

            with st.expander("ğŸ” ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸(ê·¼ê±°)", expanded=False):
                st.write(ctx)

            with st.spinner("LLM ìƒì„±(Bedrock)â€¦"):
                prompt = make_question_prompt(query, ctx)
                answer = invoke_llm(prompt, model_id=LLM_MODEL_ID,
                                    max_tokens=int(max_tokens), temperature=float(temperature))

            st.subheader("ğŸ§© ìƒì„± ê²°ê³¼")
            st.markdown(answer)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜: {e}")
